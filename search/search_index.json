{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the IEEE ML Bootcamp Companion Website! Here you will find all of the documentation, and tutorials that accompany the workshop material and powerpoints! Here you can find extra resources, and solution code to everything that we will be covered in the workshops. Workshop Schedule # Workshop Title Date Location 1 Introduction to ML and Setup February 2 nd , 2020 EBU-1 2315 2 Classical ML Algorithms and NLP February 9 th , 2020 EBU-1 2315 2 Neural Networks and Computer Vision February 16 th , 2020 EBU-1 2315 About This bootcamp is developed by the UCSD IEEE Student Branch Website Facebook Newsletter Bootcamp Staff Dillon Hicks Angela Liu - Technical Chair Jaden Padua - Technical Chair","title":"Home"},{"location":"#welcome-to-the-ieee-ml-bootcamp-companion-website","text":"Here you will find all of the documentation, and tutorials that accompany the workshop material and powerpoints! Here you can find extra resources, and solution code to everything that we will be covered in the workshops.","title":"Welcome to the IEEE ML Bootcamp Companion Website!"},{"location":"#workshop-schedule","text":"# Workshop Title Date Location 1 Introduction to ML and Setup February 2 nd , 2020 EBU-1 2315 2 Classical ML Algorithms and NLP February 9 th , 2020 EBU-1 2315 2 Neural Networks and Computer Vision February 16 th , 2020 EBU-1 2315","title":"Workshop Schedule"},{"location":"#about","text":"This bootcamp is developed by the UCSD IEEE Student Branch Website Facebook Newsletter Bootcamp Staff Dillon Hicks Angela Liu - Technical Chair Jaden Padua - Technical Chair","title":"About"},{"location":"0. Setup/","text":"Getting Started Preface This workshop series is intended to be fast paced introduction into the theory and applications of Machine Learning. It is assumed that you have a relatively strong basis of Linear Algebra, and Programming to be able to complete this bootcamp. Just to give an overview of what you will learn (and what you won't learn this time around): ML Subjects we will cover: Classical Linear and Clustering ML Algorithms Deep Learning and Neural Networks Natural Language Processing Basics Computer Vision Basics ML Subjects we won't cover: Probabilistic Models (Naive Bayes) Ensemble Models (Random Forest, Adaboost, xgboost) Reinforcement Learning (Q Learning, Markov Decision Processes) Deployment of Machine Learning Models Question Even though we aren't covering some of the previous topics, there are extra resources and other ML information in the Additional Resources section in the sidebar. Also if you would like workshops on the previous topics, make sure to let us know! Anaconda Summary Anaconda makes it very easy to install many tools that are used widely in Machine Learning and Data Science Workflows. It includes many of the Python libraries and tools that we will use in this Bootcamp including but not limited to: Jupyter Notebook Numpy Pandas scikit-learn OpenCV Tensorflow We will be using Anaconda for the entirety of this bootcamp series for package management Follow the steps below for your following operating system to install Anaconda. https://docs.anaconda.com/anaconda/install/ Jupyter Notebook Once you have installed Anaconda on your system, launch Jupyter Notebook from Anaconda Navigator. Jupyter notebook is a program that allows us to run our Python code in our browser in the form of notebooks. These notebooks can include markdown code, display graphs and tables, and run code through cells, meaning you don't have to fully run your code everytime you make an addition. Jupyter notebook may get hard to get used to for the Jupyter novice, but will enable you to be much more productive with your Python code. Tip Jupyter notebook will open in your computer's base directory, so if you want to open Jupyter notebook in a different folder by default, follow this steps to change your Jupyter default directory: https://stackoverflow.com/questions/15680463/change-ipython-jupyter-notebook-working-directory","title":"Getting Started"},{"location":"0. Setup/#getting-started","text":"","title":"Getting Started"},{"location":"0. Setup/#preface","text":"This workshop series is intended to be fast paced introduction into the theory and applications of Machine Learning. It is assumed that you have a relatively strong basis of Linear Algebra, and Programming to be able to complete this bootcamp. Just to give an overview of what you will learn (and what you won't learn this time around): ML Subjects we will cover: Classical Linear and Clustering ML Algorithms Deep Learning and Neural Networks Natural Language Processing Basics Computer Vision Basics ML Subjects we won't cover: Probabilistic Models (Naive Bayes) Ensemble Models (Random Forest, Adaboost, xgboost) Reinforcement Learning (Q Learning, Markov Decision Processes) Deployment of Machine Learning Models Question Even though we aren't covering some of the previous topics, there are extra resources and other ML information in the Additional Resources section in the sidebar. Also if you would like workshops on the previous topics, make sure to let us know!","title":"Preface"},{"location":"0. Setup/#anaconda","text":"Summary Anaconda makes it very easy to install many tools that are used widely in Machine Learning and Data Science Workflows. It includes many of the Python libraries and tools that we will use in this Bootcamp including but not limited to: Jupyter Notebook Numpy Pandas scikit-learn OpenCV Tensorflow We will be using Anaconda for the entirety of this bootcamp series for package management Follow the steps below for your following operating system to install Anaconda. https://docs.anaconda.com/anaconda/install/","title":"Anaconda"},{"location":"0. Setup/#jupyter-notebook","text":"Once you have installed Anaconda on your system, launch Jupyter Notebook from Anaconda Navigator. Jupyter notebook is a program that allows us to run our Python code in our browser in the form of notebooks. These notebooks can include markdown code, display graphs and tables, and run code through cells, meaning you don't have to fully run your code everytime you make an addition. Jupyter notebook may get hard to get used to for the Jupyter novice, but will enable you to be much more productive with your Python code. Tip Jupyter notebook will open in your computer's base directory, so if you want to open Jupyter notebook in a different folder by default, follow this steps to change your Jupyter default directory: https://stackoverflow.com/questions/15680463/change-ipython-jupyter-notebook-working-directory","title":"Jupyter Notebook"},{"location":"1. Basics/","text":"Before we talk about any machine learning, let's talk about the main tools that we are going to be using for this workshop in the bootcamp: NumPy and Pandas Tools: NumPy and Pandas Numpy Numpy is a library that includes many tools for mathematical computations, including a computationally efficient ndarray (much faster than python lists) 1 In addition, many mathematical functions useful in linear algebra are included in NumPy with the numpy.linalg functions. These functions will prove to be pretty useful in the rest of the workshop. If you are familiar with using MATLAB from MATH18, then you should be pretty familiar with the functions that NumPy provides. Pandas Pandas is a library that includes data structures and data analysis tools for data munging and preparation. Pandas makes it relatively easy to interact and filter through data, with nice convenience functions for plotting and exporting data to SQL databases or CSV's. Faq I heard that python is a very slow language. If it's so slow, why do we use python for such intensive tasks such as Machine Learning and Data Analysis? While it is true that Python in itself is a very slow language, most of the tools that we are using to implement ML algorithms don't use Python to do calculations. Libraries such as Numpy and Tensorflow respectively have C and C++ backends 2 3 , allowing them to be very fast. You can even uses other libraries such as Intel MKL to have hardware optimized linear algebra operations in Numpy if you are especially a speed demon. The Challenge Say you are a data scientist working for an investment firm. A client wants to invest their money into real estate, buying homes in a specific neighborhood to airbnb. However, none of the homes are for sale, and the people living inside the homes won't let you appraise their homes because they hate airbnb. How do you find an estimate of their home price using data available to you? Linear Regression : What is Regression? Regression is a powerful tool that can often be used for predicting and . With the challenge above, we want to predict the house Regression is Continuous , meaning that it is trying to predict values from the variables, or features that your data has. For the example we are using below, rows 1-x make up the features that can be used to decide a predictor below. Definition Linear Regression is the process of taking a line and fitting it to data. What we mean by \"fitting\" is that that we want to make a line that \"approximates\" the data (We'll get more into what we mean by this in a bit). Here is an example below with two dimensions (for example, if your dataset has 2 variables). From the points below, we can predict the value for y with a value for x with the line given. Essentially, we want to find the weights w, such that the dot product between w and an input vector x (for example, an input sample), can be predicted from these weights. Or, we want to find an equation that when our features are plugged in, we will get a y value that can be used to predict our final values w_1x_1 \\; \\cdot \\; w_2x_2 \\; \\cdot \\; [\\dots] \\; \\cdot \\; w_nx_n \\; = \\; \\hat{y} w_1x_1 \\; \\cdot \\; w_2x_2 \\; \\cdot \\; [\\dots] \\; \\cdot \\; w_nx_n \\; = \\; \\hat{y} But, what exactly is an \"optimal solution\", and how do we get one? We'll get to how to find this solution, but let's set up our goal to find the most optimal approximate solution, otherwise known as setting up our objective function . We want to optimize the above equation so that it minimizes our objective function . In other words, we want to find the equation such that our loss is minimized. In fancy mathematical writing we want to minimize: \\begin{Vmatrix} y - Ax \\end{Vmatrix} \\begin{Vmatrix} y - Ax \\end{Vmatrix} Info In much of machine learning, the goal is simply to reduce the error between the predictions and the true values, such as what we are doing in the challenge above. Just remember, that an objective function is a means to an end, not a means to an end. As we show below, there are many way to optimize a function using Gradient Descent Let's say we want to find the minimize a function, for example x^2 x^2 . We can easily find the minimum of this function (remember the second derivative rule in calculus?), but how can we do it for more complicated functions that are harder to differentiate like our objective function? This problem will get very hard , and very computationally intensive . To get over this hurdle of finding the exact minimum, we can instead approximate the minimum, using an algorithm called Gradient Descent . Imagine you have a ball and place it on a slope. It will move downwards, with the direction opposite of the gradient of the slope (remember the gradient is in the direction of ascent ). We can think of gradient descent as something similar. Here we will move our current approximation of the minimum, towards the gradient. When the gradient gets small enough (in other words, when the slope is near zero at a minimum) or when we have moved our approximation for enough epochs , then we set the current approximation as our final value. However, calculating the gradient of the cost function is a costly procedure, as it has to be done with each weight of your model. If we wanted to do this mathematically, we would have to calculate the equation below to find the derivative for each feature and each sample , which would kill any computer. \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } Instead of doing that, let's simply define the gradient as the difference between our predictions from the current iteration of gradient descent and the true values, multiplied by our sample features. This will get an approximation of the change over each feature the loss. \\nabla f(X) = -X(values-predictions) \\nabla f(X) = -X(values-predictions) Here is psuedocode for Gradient Descent below, courtesy of CS231n: Gradient Descent Pseudocode 4 1 2 3 4 5 # Vanilla Gradient Descent while True : weights_grad = evaluate_gradient ( loss_fun , data , weights ) weights += - step_size * weights_grad # perform parameter update Stochastic Gradient Descent However, gradient descent has a huge drawback: it is computationally ineffecient . For every epoch, the gradient is calculated over all training samples. This is costly, and is unfeasible for most datasets. We can improve this by using Stochastic Gradient Descent (SGD) , a modification of the standard Gradient Descent algorithm. Stochastic Gradient Descent tries to solve the previous problem by only calculating the gradient for a single randomly sampled point at a time , hence the word Stochastic . Thus, this allows to update the weights of our model much quicker, allowing SGD to converge to a minima much quicker than in standard Gradient Descent. Although this optimization method is \"noisier\" it can often be much quicker to optimize a solution (illustrated below). We can reduce this use by calculating the gradient on batches of data instead of a single sample with Minibatch Gradient Descent . This can strike a balance can help reduce noise, but also allow us to still converge on our minima quickly. This noise is not always a negative trait though, as it can in fact possibly find us a more optimal solution for our regression. This additional noise can allow our solution to essentially \"hop out\" of local minima, allowing for the search of a more optimal minima. 5 This can be especially useful when you have a complicated loss landscape with many local minima, such as those for complicated neural networks: Info Approximating the weights of this equation using SGD is one way to solve linear systems, but it is not the only way. One other method that may seem familiar to you is solving the closed form system of equations using linear algebra. I've linked this additional method in the Extras page . Linear Regression in Numpy Alright, let's finally do some coding! Initialization O Evaluating the Gradient Updating Weights Linear Regression Binary Classifier Regression is continuous, so how can we turn this into something that is discrete. In other words, how can we go from our ML model predicting values, to predicting categories? One simple way, is simply changing your prediction value from a continuous variable, to a discrete variable. In other words, we can simply Conclusion So in conclusion, we learned a few things from this workshop 1. How we can use concepts learned in Linear Algebra to create a simple Machine Learning Algorithm What Regression is, how it is applied Basics of Python ndarrays vs. Lists https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference \u21a9 Numpy Internals https://docs.scipy.org/doc/numpy-1.13.0/reference/internals.html \u21a9 Tensorflow Core C++ API https://www.tensorflow.org/api_docs/cc/group/core \u21a9 CS231n: Optimization http://cs231n.github.io/optimization-1/#optimization \u21a9 SGD and Local Minima https://leon.bottou.org/publications/pdf/nimes-1991.pdf \u21a9 Neural Network Loss Landscapes https://www.cs.umd.edu/~tomg/projects/landscapes/ \u21a9","title":"1. Basics"},{"location":"1. Basics/#tools-numpy-and-pandas","text":"","title":"Tools: NumPy and Pandas"},{"location":"1. Basics/#numpy","text":"Numpy is a library that includes many tools for mathematical computations, including a computationally efficient ndarray (much faster than python lists) 1 In addition, many mathematical functions useful in linear algebra are included in NumPy with the numpy.linalg functions. These functions will prove to be pretty useful in the rest of the workshop. If you are familiar with using MATLAB from MATH18, then you should be pretty familiar with the functions that NumPy provides.","title":"Numpy"},{"location":"1. Basics/#pandas","text":"Pandas is a library that includes data structures and data analysis tools for data munging and preparation. Pandas makes it relatively easy to interact and filter through data, with nice convenience functions for plotting and exporting data to SQL databases or CSV's. Faq I heard that python is a very slow language. If it's so slow, why do we use python for such intensive tasks such as Machine Learning and Data Analysis? While it is true that Python in itself is a very slow language, most of the tools that we are using to implement ML algorithms don't use Python to do calculations. Libraries such as Numpy and Tensorflow respectively have C and C++ backends 2 3 , allowing them to be very fast. You can even uses other libraries such as Intel MKL to have hardware optimized linear algebra operations in Numpy if you are especially a speed demon.","title":"Pandas"},{"location":"1. Basics/#the-challenge","text":"Say you are a data scientist working for an investment firm. A client wants to invest their money into real estate, buying homes in a specific neighborhood to airbnb. However, none of the homes are for sale, and the people living inside the homes won't let you appraise their homes because they hate airbnb. How do you find an estimate of their home price using data available to you?","title":"The Challenge"},{"location":"1. Basics/#linear-regression","text":"","title":"Linear Regression:"},{"location":"1. Basics/#what-is-regression","text":"Regression is a powerful tool that can often be used for predicting and . With the challenge above, we want to predict the house Regression is Continuous , meaning that it is trying to predict values from the variables, or features that your data has. For the example we are using below, rows 1-x make up the features that can be used to decide a predictor below.","title":"What is Regression?"},{"location":"1. Basics/#definition","text":"Linear Regression is the process of taking a line and fitting it to data. What we mean by \"fitting\" is that that we want to make a line that \"approximates\" the data (We'll get more into what we mean by this in a bit). Here is an example below with two dimensions (for example, if your dataset has 2 variables). From the points below, we can predict the value for y with a value for x with the line given. Essentially, we want to find the weights w, such that the dot product between w and an input vector x (for example, an input sample), can be predicted from these weights. Or, we want to find an equation that when our features are plugged in, we will get a y value that can be used to predict our final values w_1x_1 \\; \\cdot \\; w_2x_2 \\; \\cdot \\; [\\dots] \\; \\cdot \\; w_nx_n \\; = \\; \\hat{y} w_1x_1 \\; \\cdot \\; w_2x_2 \\; \\cdot \\; [\\dots] \\; \\cdot \\; w_nx_n \\; = \\; \\hat{y} But, what exactly is an \"optimal solution\", and how do we get one? We'll get to how to find this solution, but let's set up our goal to find the most optimal approximate solution, otherwise known as setting up our objective function . We want to optimize the above equation so that it minimizes our objective function . In other words, we want to find the equation such that our loss is minimized. In fancy mathematical writing we want to minimize: \\begin{Vmatrix} y - Ax \\end{Vmatrix} \\begin{Vmatrix} y - Ax \\end{Vmatrix} Info In much of machine learning, the goal is simply to reduce the error between the predictions and the true values, such as what we are doing in the challenge above. Just remember, that an objective function is a means to an end, not a means to an end. As we show below, there are many way to optimize a function using","title":"Definition"},{"location":"1. Basics/#gradient-descent","text":"Let's say we want to find the minimize a function, for example x^2 x^2 . We can easily find the minimum of this function (remember the second derivative rule in calculus?), but how can we do it for more complicated functions that are harder to differentiate like our objective function? This problem will get very hard , and very computationally intensive . To get over this hurdle of finding the exact minimum, we can instead approximate the minimum, using an algorithm called Gradient Descent . Imagine you have a ball and place it on a slope. It will move downwards, with the direction opposite of the gradient of the slope (remember the gradient is in the direction of ascent ). We can think of gradient descent as something similar. Here we will move our current approximation of the minimum, towards the gradient. When the gradient gets small enough (in other words, when the slope is near zero at a minimum) or when we have moved our approximation for enough epochs , then we set the current approximation as our final value. However, calculating the gradient of the cost function is a costly procedure, as it has to be done with each weight of your model. If we wanted to do this mathematically, we would have to calculate the equation below to find the derivative for each feature and each sample , which would kill any computer. \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } Instead of doing that, let's simply define the gradient as the difference between our predictions from the current iteration of gradient descent and the true values, multiplied by our sample features. This will get an approximation of the change over each feature the loss. \\nabla f(X) = -X(values-predictions) \\nabla f(X) = -X(values-predictions) Here is psuedocode for Gradient Descent below, courtesy of CS231n: Gradient Descent Pseudocode 4 1 2 3 4 5 # Vanilla Gradient Descent while True : weights_grad = evaluate_gradient ( loss_fun , data , weights ) weights += - step_size * weights_grad # perform parameter update","title":"Gradient Descent"},{"location":"1. Basics/#stochastic-gradient-descent","text":"However, gradient descent has a huge drawback: it is computationally ineffecient . For every epoch, the gradient is calculated over all training samples. This is costly, and is unfeasible for most datasets. We can improve this by using Stochastic Gradient Descent (SGD) , a modification of the standard Gradient Descent algorithm. Stochastic Gradient Descent tries to solve the previous problem by only calculating the gradient for a single randomly sampled point at a time , hence the word Stochastic . Thus, this allows to update the weights of our model much quicker, allowing SGD to converge to a minima much quicker than in standard Gradient Descent. Although this optimization method is \"noisier\" it can often be much quicker to optimize a solution (illustrated below). We can reduce this use by calculating the gradient on batches of data instead of a single sample with Minibatch Gradient Descent . This can strike a balance can help reduce noise, but also allow us to still converge on our minima quickly. This noise is not always a negative trait though, as it can in fact possibly find us a more optimal solution for our regression. This additional noise can allow our solution to essentially \"hop out\" of local minima, allowing for the search of a more optimal minima. 5 This can be especially useful when you have a complicated loss landscape with many local minima, such as those for complicated neural networks: Info Approximating the weights of this equation using SGD is one way to solve linear systems, but it is not the only way. One other method that may seem familiar to you is solving the closed form system of equations using linear algebra. I've linked this additional method in the Extras page .","title":"Stochastic Gradient Descent"},{"location":"1. Basics/#linear-regression-in-numpy","text":"Alright, let's finally do some coding!","title":"Linear Regression in Numpy"},{"location":"1. Basics/#initialization","text":"O","title":"Initialization"},{"location":"1. Basics/#evaluating-the-gradient","text":"","title":"Evaluating the Gradient"},{"location":"1. Basics/#updating-weights","text":"","title":"Updating Weights"},{"location":"1. Basics/#linear-regression-binary-classifier","text":"Regression is continuous, so how can we turn this into something that is discrete. In other words, how can we go from our ML model predicting values, to predicting categories? One simple way, is simply changing your prediction value from a continuous variable, to a discrete variable. In other words, we can simply","title":"Linear Regression Binary Classifier"},{"location":"1. Basics/#conclusion","text":"So in conclusion, we learned a few things from this workshop 1. How we can use concepts learned in Linear Algebra to create a simple Machine Learning Algorithm What Regression is, how it is applied Basics of Python ndarrays vs. Lists https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference \u21a9 Numpy Internals https://docs.scipy.org/doc/numpy-1.13.0/reference/internals.html \u21a9 Tensorflow Core C++ API https://www.tensorflow.org/api_docs/cc/group/core \u21a9 CS231n: Optimization http://cs231n.github.io/optimization-1/#optimization \u21a9 SGD and Local Minima https://leon.bottou.org/publications/pdf/nimes-1991.pdf \u21a9 Neural Network Loss Landscapes https://www.cs.umd.edu/~tomg/projects/landscapes/ \u21a9","title":"Conclusion"},{"location":"2. Classical ML and NLP/","text":"What is Classical Machine Learning? In this bootcamp, when we refer to Classical Machine Learning Algorithms, we simply mean algorithms that are not based on Deep Learning or Neural Network based learning methods. These algorithms have been used for decades, far before the current hype of Machine Learning and Artificial Intelligence Some examples of Classical Machine Learning Algorithms include but are not limited to: Supervised Learning : Naive Bayes, Linear Regression, Logistic Regression, Ridge Regression, K Nearest Neighbors, Support Vector Machine Unsupervised Learning : Mean-shift, DBSCAN, K-Means Info DEFINITION DETOUR : Unsupervised and Supervised Learning Supervised : Supervised learning is unique in that it has training data. Unsupervised : Since Classical Machine Learning Algorithms are often outclassed by Deep Learning algorthims, why should we even use them? The answer is more clear than you think. Simplicity . Classical Machine Learning Methods are often easier to explain and more computationally efficient that Deep Learning Based Approaches, allowing them to be deployed much easier and cheaper than their neural network-based counterparts. In addition, most classical algorithms run directly on the CPU, voiding the need for more costly GPU's. Perform Better with Less Data . Classical Machine Learning algorithms don't need as much data to get good predictions, and in many cases can perform better than neural networks with limited data. scikit-learn Scikit-learn includes off the shelf machine learning algorithms The Challenge You recently got fired from you job as a data scientist from your job as because they discovered that you lied on your resume and couldn't implement linear regression from scratch. You decide to pivot and work for a newspaper, who is trying to figure out the category of each news article to deliver tailored ads. In addition, they want to create a recommender system to recommend articles to readers. How do you do this? The dataset Natural Language Processing In the past workshop, we were dealing with almost exclusively numerical data. This made things pretty easy for us. This allowed us to put these numbers into vectors, which could easily work with our machine learning algorithms. However, with text data, if you wanted to use it with machine learning, we have to answer one main question How can we turn our text data into features that our machine learning model can use? Feature Engineering Feature Engineering is the process of making Features from our data. This is especia Challenge: Categorization Challenge: Recommender System","title":"2. Classical ML and NLP"},{"location":"2. Classical ML and NLP/#what-is-classical-machine-learning","text":"In this bootcamp, when we refer to Classical Machine Learning Algorithms, we simply mean algorithms that are not based on Deep Learning or Neural Network based learning methods. These algorithms have been used for decades, far before the current hype of Machine Learning and Artificial Intelligence Some examples of Classical Machine Learning Algorithms include but are not limited to: Supervised Learning : Naive Bayes, Linear Regression, Logistic Regression, Ridge Regression, K Nearest Neighbors, Support Vector Machine Unsupervised Learning : Mean-shift, DBSCAN, K-Means Info DEFINITION DETOUR : Unsupervised and Supervised Learning Supervised : Supervised learning is unique in that it has training data. Unsupervised : Since Classical Machine Learning Algorithms are often outclassed by Deep Learning algorthims, why should we even use them? The answer is more clear than you think. Simplicity . Classical Machine Learning Methods are often easier to explain and more computationally efficient that Deep Learning Based Approaches, allowing them to be deployed much easier and cheaper than their neural network-based counterparts. In addition, most classical algorithms run directly on the CPU, voiding the need for more costly GPU's. Perform Better with Less Data . Classical Machine Learning algorithms don't need as much data to get good predictions, and in many cases can perform better than neural networks with limited data.","title":"What is Classical Machine Learning?"},{"location":"2. Classical ML and NLP/#scikit-learn","text":"Scikit-learn includes off the shelf machine learning algorithms","title":"scikit-learn"},{"location":"2. Classical ML and NLP/#the-challenge","text":"You recently got fired from you job as a data scientist from your job as because they discovered that you lied on your resume and couldn't implement linear regression from scratch. You decide to pivot and work for a newspaper, who is trying to figure out the category of each news article to deliver tailored ads. In addition, they want to create a recommender system to recommend articles to readers. How do you do this?","title":"The Challenge"},{"location":"2. Classical ML and NLP/#the-dataset","text":"","title":"The dataset"},{"location":"2. Classical ML and NLP/#natural-language-processing","text":"In the past workshop, we were dealing with almost exclusively numerical data. This made things pretty easy for us. This allowed us to put these numbers into vectors, which could easily work with our machine learning algorithms. However, with text data, if you wanted to use it with machine learning, we have to answer one main question How can we turn our text data into features that our machine learning model can use?","title":"Natural Language Processing"},{"location":"2. Classical ML and NLP/#feature-engineering","text":"Feature Engineering is the process of making Features from our data. This is especia","title":"Feature Engineering"},{"location":"2. Classical ML and NLP/#challenge-categorization","text":"","title":"Challenge: Categorization"},{"location":"2. Classical ML and NLP/#challenge-recommender-system","text":"","title":"Challenge: Recommender System"},{"location":"3. Neural Networks and Computer Vision/","text":"What are Neural Networks Deep Learning Faq ***What is the","title":"3. Neural Networks and Computer Vision"},{"location":"3. Neural Networks and Computer Vision/#what-are-neural-networks","text":"","title":"What are Neural Networks"},{"location":"3. Neural Networks and Computer Vision/#deep-learning","text":"Faq ***What is the","title":"Deep Learning"},{"location":"Additional Resources/","text":"Slideshows for Previous Workshops: Additional Machine Learning Resources if you are interested: Stanford Courses: These courses are taught by some of the world's most prevalent experts in machine learning, and include all information from lecture videos and notes to get you started. I highly reccomend these to those starting out. CS 230: Deep Learning - Stanford University CS 224N: Natural Language Processing with Deep Learning - Stanford University CS 231n: Convolutional Neural networks for Visual Recognition - Stanford University Online Courses Udacity Intro to Machine Learning fast.ai Practical Deep Learning for Coders Google Machine Learning Crash Course Machine Learning Videos sentdex ML tutorials","title":"Additional Resources"},{"location":"Additional Resources/#slideshows-for-previous-workshops","text":"","title":"Slideshows for Previous Workshops:"},{"location":"Additional Resources/#additional-machine-learning-resources-if-you-are-interested","text":"","title":"Additional Machine Learning Resources if you are interested:"},{"location":"Additional Resources/#stanford-courses","text":"These courses are taught by some of the world's most prevalent experts in machine learning, and include all information from lecture videos and notes to get you started. I highly reccomend these to those starting out. CS 230: Deep Learning - Stanford University CS 224N: Natural Language Processing with Deep Learning - Stanford University CS 231n: Convolutional Neural networks for Visual Recognition - Stanford University","title":"Stanford Courses:"},{"location":"Additional Resources/#online-courses","text":"Udacity Intro to Machine Learning fast.ai Practical Deep Learning for Coders Google Machine Learning Crash Course","title":"Online Courses"},{"location":"Additional Resources/#machine-learning-videos","text":"sentdex ML tutorials","title":"Machine Learning Videos"},{"location":"Extras/","text":"Workshop 1 - Linear Algebra Method of Solving the Problem of Least Squares Some of you may have asked, why would you use SGD when other methods for solving for least squares exist, such as simply finding the psuedoinverse? This is a very good question, as solving for the least squares problem can be easily done through the psuedoinverse. In the section below, I'll explain How you can use the psuedoinverse to find the weights of a linear least squares problem Why this method is in many cases can be worse than using SGD to find the weights. Bridging the Gap: Linear Algbra and Least Squares Let's get into the math of it. Say if we wanted to find the line between two points in a two dimensional space i.e. we only want to fit a line to two points in our dataset. This would be easy, simply solve the system of linear equations to find a line that intersects these two points. In other words, we can just row reduce the matrix below. rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} or, solving for x, the parameters of the line which predicts y, is the following: x = A^{-1}y x = A^{-1}y Therefore, we can get a line such that mx = y What about if we have so many vectors that we aren't able to make a line that intersects these points, for example with the dataset we've shown above? Row reducing this matrix is impossible as the resulting augmented matrix will have free row variables, meaning there is no solution and the matrix is not invertible (Keep in mind that a matrix needs to be invertible in order to find x). rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} This type of matrix is known as a tall, or overdetermined matrix. However, even though there is no exact solution because the matrix isn't invertible, that doesn't mean that there isn't an approximate solution that we can solve for. Psuedoinverse: Solving Overdetermined Matrices One way to solve for this is finding the psuedoinverse of the matrix to solve for the closest solution. The pseudoinverse is found with the following: We want to invert the right side in order to isolate x. If A was square, we could simply multiply both sides by the inverse of the original matrix to find our weights, x. However, we can make the matrix square through a simple algebraic transformation. We can multiply both sides by the matrix transposed, A^T A^T , such that A^TAx = A^Ty A^TAx = A^Ty We can then multiply both sides by the inverse of the new matrix on the left, (A^TA)^{-1} (A^TA)^{-1} such that x = (A^TA)^{-1}(A^TA)y x = (A^TA)^{-1}(A^TA)y This will get us the weights of the line, x, that can be used to predict values. Summary Just to give a bit of explanations for the variables we used above: \\hat{y} \\hat{y} : The predicted value that approximates y for an input sample y y : The true value for each sample that we are trying to predict A A : The input matrix. i.e. the features as the columns with each row being a single sample of your data. x x : The weights of the final approximate least squares equation","title":"Extras"},{"location":"Extras/#workshop-1-linear-algebra-method-of-solving-the-problem-of-least-squares","text":"Some of you may have asked, why would you use SGD when other methods for solving for least squares exist, such as simply finding the psuedoinverse? This is a very good question, as solving for the least squares problem can be easily done through the psuedoinverse. In the section below, I'll explain How you can use the psuedoinverse to find the weights of a linear least squares problem Why this method is in many cases can be worse than using SGD to find the weights.","title":"Workshop 1 - Linear Algebra Method of Solving the Problem of Least Squares"},{"location":"Extras/#bridging-the-gap-linear-algbra-and-least-squares","text":"Let's get into the math of it. Say if we wanted to find the line between two points in a two dimensional space i.e. we only want to fit a line to two points in our dataset. This would be easy, simply solve the system of linear equations to find a line that intersects these two points. In other words, we can just row reduce the matrix below. rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} or, solving for x, the parameters of the line which predicts y, is the following: x = A^{-1}y x = A^{-1}y Therefore, we can get a line such that mx = y What about if we have so many vectors that we aren't able to make a line that intersects these points, for example with the dataset we've shown above? Row reducing this matrix is impossible as the resulting augmented matrix will have free row variables, meaning there is no solution and the matrix is not invertible (Keep in mind that a matrix needs to be invertible in order to find x). rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} This type of matrix is known as a tall, or overdetermined matrix. However, even though there is no exact solution because the matrix isn't invertible, that doesn't mean that there isn't an approximate solution that we can solve for.","title":"Bridging the Gap: Linear Algbra and Least Squares"},{"location":"Extras/#psuedoinverse-solving-overdetermined-matrices","text":"One way to solve for this is finding the psuedoinverse of the matrix to solve for the closest solution. The pseudoinverse is found with the following: We want to invert the right side in order to isolate x. If A was square, we could simply multiply both sides by the inverse of the original matrix to find our weights, x. However, we can make the matrix square through a simple algebraic transformation. We can multiply both sides by the matrix transposed, A^T A^T , such that A^TAx = A^Ty A^TAx = A^Ty We can then multiply both sides by the inverse of the new matrix on the left, (A^TA)^{-1} (A^TA)^{-1} such that x = (A^TA)^{-1}(A^TA)y x = (A^TA)^{-1}(A^TA)y This will get us the weights of the line, x, that can be used to predict values. Summary Just to give a bit of explanations for the variables we used above: \\hat{y} \\hat{y} : The predicted value that approximates y for an input sample y y : The true value for each sample that we are trying to predict A A : The input matrix. i.e. the features as the columns with each row being a single sample of your data. x x : The weights of the final approximate least squares equation","title":"Psuedoinverse: Solving Overdetermined Matrices"}]}