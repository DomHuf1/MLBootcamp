{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the IEEE ML Bootcamp Companion Website! Here you will find all of the documentation, and tutorials that accompany the workshop material and powerpoints! Here you can find extra resources, and solution code to everything that we will be covered in the workshops. Workshop Schedule # Workshop Title Date Location 1 Introduction to ML and Setup February 2 nd , 2020 EBU-1 2315 2 Classical ML Algorithms and NLP February 9 th , 2020 EBU-1 2315 2 Neural Networks and Computer Vision February 16 th , 2020 EBU-1 2315 About This bootcamp is developed by the UCSD IEEE Student Branch Website Facebook Newsletter Bootcamp Staff Dillon Hicks Angela Liu - Technical Chair Jaden Padua - Technical Chair","title":"Home"},{"location":"#welcome-to-the-ieee-ml-bootcamp-companion-website","text":"Here you will find all of the documentation, and tutorials that accompany the workshop material and powerpoints! Here you can find extra resources, and solution code to everything that we will be covered in the workshops.","title":"Welcome to the IEEE ML Bootcamp Companion Website!"},{"location":"#workshop-schedule","text":"# Workshop Title Date Location 1 Introduction to ML and Setup February 2 nd , 2020 EBU-1 2315 2 Classical ML Algorithms and NLP February 9 th , 2020 EBU-1 2315 2 Neural Networks and Computer Vision February 16 th , 2020 EBU-1 2315","title":"Workshop Schedule"},{"location":"#about","text":"This bootcamp is developed by the UCSD IEEE Student Branch Website Facebook Newsletter Bootcamp Staff Dillon Hicks Angela Liu - Technical Chair Jaden Padua - Technical Chair","title":"About"},{"location":"0. Setup/","text":"Getting Started Preface This workshop series is intended to be fast paced introduction into the theory and applications of Machine Learning. It is assumed that you have a relatively strong basis of Linear Algebra, and Programming to be able to complete this bootcamp. Just to give an overview of what you will learn (and what you won't learn this time around): ML Subjects we will cover: Classical Linear and Clustering ML Algorithms Deep Learning and Neural Networks Natural Language Processing Basics Computer Vision Basics ML Subjects we won't cover: Probabilistic Models (Naive Bayes) Ensemble Models (Random Forest, Adaboost, xgboost) Reinforcement Learning (Q Learning, Markov Decision Processes) Deployment of Machine Learning Models Question Even though we aren't covering some of the previous topics, there are extra resources and other ML information in the Additional Resources section in the sidebar. Also if you would like workshops on the previous topics, make sure to let us know! Anaconda Summary Anaconda makes it very easy to install many tools that are used widely in Machine Learning and Data Science Workflows. It includes many of the Python libraries and tools that we will use in this Bootcamp including but not limited to: Jupyter Notebook Numpy Pandas scikit-learn OpenCV Tensorflow We will be using Anaconda for the entirety of this bootcamp series for package management Follow the steps below for your following operating system to install Anaconda. https://docs.anaconda.com/anaconda/install/ Jupyter Notebook Once you have installed Anaconda on your system, launch Jupyter Notebook from Anaconda Navigator. Jupyter notebook is a program that allows us to run our Python code in our browser in the form of notebooks. These notebooks can include markdown code, display graphs and tables, and run code through cells, meaning you don't have to fully run your code everytime you make an addition. Jupyter notebook may get hard to get used to for the Jupyter novice, but will enable you to be much more productive with your Python code. Tip Jupyter notebook will open in your computer's base directory, so if you want to open Jupyter notebook in a different folder by default, follow this steps to change your Jupyter default directory: https://stackoverflow.com/questions/15680463/change-ipython-jupyter-notebook-working-directory","title":"Getting Started"},{"location":"0. Setup/#getting-started","text":"","title":"Getting Started"},{"location":"0. Setup/#preface","text":"This workshop series is intended to be fast paced introduction into the theory and applications of Machine Learning. It is assumed that you have a relatively strong basis of Linear Algebra, and Programming to be able to complete this bootcamp. Just to give an overview of what you will learn (and what you won't learn this time around): ML Subjects we will cover: Classical Linear and Clustering ML Algorithms Deep Learning and Neural Networks Natural Language Processing Basics Computer Vision Basics ML Subjects we won't cover: Probabilistic Models (Naive Bayes) Ensemble Models (Random Forest, Adaboost, xgboost) Reinforcement Learning (Q Learning, Markov Decision Processes) Deployment of Machine Learning Models Question Even though we aren't covering some of the previous topics, there are extra resources and other ML information in the Additional Resources section in the sidebar. Also if you would like workshops on the previous topics, make sure to let us know!","title":"Preface"},{"location":"0. Setup/#anaconda","text":"Summary Anaconda makes it very easy to install many tools that are used widely in Machine Learning and Data Science Workflows. It includes many of the Python libraries and tools that we will use in this Bootcamp including but not limited to: Jupyter Notebook Numpy Pandas scikit-learn OpenCV Tensorflow We will be using Anaconda for the entirety of this bootcamp series for package management Follow the steps below for your following operating system to install Anaconda. https://docs.anaconda.com/anaconda/install/","title":"Anaconda"},{"location":"0. Setup/#jupyter-notebook","text":"Once you have installed Anaconda on your system, launch Jupyter Notebook from Anaconda Navigator. Jupyter notebook is a program that allows us to run our Python code in our browser in the form of notebooks. These notebooks can include markdown code, display graphs and tables, and run code through cells, meaning you don't have to fully run your code everytime you make an addition. Jupyter notebook may get hard to get used to for the Jupyter novice, but will enable you to be much more productive with your Python code. Tip Jupyter notebook will open in your computer's base directory, so if you want to open Jupyter notebook in a different folder by default, follow this steps to change your Jupyter default directory: https://stackoverflow.com/questions/15680463/change-ipython-jupyter-notebook-working-directory","title":"Jupyter Notebook"},{"location":"1. Basics/","text":"Before we talk about any machine learning, let's talk about the main tools that we are going to be using for this workshop in the bootcamp: NumPy and Pandas Tools: NumPy and Pandas Numpy Numpy is a library that includes many tools for mathematical computations, including a computationally efficient ndarray (much faster than python lists) 1 In addition, many mathematical functions useful in linear algebra are included in NumPy with the numpy.linalg functions. These functions will prove to be pretty useful in the rest of the workshop. If you are familiar with using MATLAB from MATH18, then you should be pretty familiar with the functions that NumPy provides. Pandas Pandas is a library that includes data structures and data analysis tools for data munging and preparation. Pandas makes it relatively easy to interact and filter through data, with nice convenience functions for plotting and exporting data to SQL databases or CSV's. Faq I heard that python is a very slow language. If it's so slow, why do we use python for such intensive tasks such as Machine Learning and Data Analysis? While it is true that Python in itself is a very slow language, most of the tools that we are using to implement ML algorithms don't use Python to do calculations. Libraries such as Numpy and Tensorflow respectively have C and C++ backends 2 3 , allowing them to be very fast. You can even uses other libraries such as Intel MKL to have hardware optimized linear algebra operations in Numpy if you are especially a speed demon. Downloads Click below to download the data and starter notebook which includes a frame for you to write your code around. Click to Download Dataset Click to Download Starter Code The Challenge Say you are a data scientist working for an investment firm. A client wants to invest their money into california real estate, buying homes in a specific block to airbnb. However, none of the homes are for sale, and the people living inside the homes won't let you appraise their homes because they hate airbnb. How do you find an estimate of their home price using data available to you? The Dataset The dataset that you are given for this challenge is a dataset on California home prices. How can you use this data to predict home prices . Below is a description of the dataset and the features you are given to predict the median home value for a block. Column title Description Range* Datatype longitude A measure of how far west a house is; a higher value is farther west Longitude values range from -180 to +180 Data set min: -124.3 Data set max: -114.3 float64 latitude A measure of how far north a house is; a higher value is farther north Latitude values range from -90 to +90 Data set min: 32.5 Data set max: 42.5 float64 housingMedianAge Median age of a house within a block; a lower number is a newer building Data set min: 1.0 Data set max: 52.0 float64 totalRooms Total number of rooms within a block Data set min: 2.0 Data set max: 37937.0 float64 totalBedrooms Total number of bedrooms within a block Data set min: 1.0 Data set max: 6445.0 float64 population Total number of people residing within a block Data set min: 3.0 Data set max: 35682.0 float64 households Total number of households, a group of people residing within a home unit, for a block Data set min: 1.0 Data set max: 6082.0 float64 medianIncome Median income for households within a block of houses (measured in tens of thousands of US Dollars) Data set min: 0.5 Data set max: 15.0 float64 medianHouseValue Median house value for households within a block (measured in US Dollars) Data set min: 14999.0 Data set max: 500001.0 float64 Info Definition: Features To clarify on the use of the word \"features\" above, features are essentially the traits of data that we use to train our machine learning model . For example, when making a model that can predict home prices, we can use all the data above to help predict the median house value for a block. The housingMedianAge , medianIncome , and all the other columns except for our predictor, medianHouseValue , are our features for our model. Linear Regression : What is Regression? Regression is a powerful tool that can often be used for predicting and values from a dataset. This is often done by creating a line or function, where the evaluation of this function can be used to predict such values. For example, with the regression below, you can predict the value of skin cancer mortality from a variable, in this case state latitude. Regression is Continuous , meaning that it is trying to predict continuous values from the variables, or features that your data has. Definition Linear Regression is the process of taking a line and fitting it to data. What we mean by \"fitting\" is that that we want to make a line that \"approximates\" the data (We'll get more into what we mean by this in a bit). The example above is with two dimensions (for example, if your dataset has 1 independent variable). Essentially, we want to find the weights w for a linear equation such that we can predict the final value from input features. Here is an example of the type of equation we are trying to set up with m independent variables. w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; \\hat{y} w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; \\hat{y} Note Representing this equation in code is hard, so it is best to represent this equation in vector (array) notation We can also put or weights and input in their vector representation, [w_1 \\; w_2 \\; w_3 \\; \\dots \\; w_m] [w_1 \\; w_2 \\; w_3 \\; \\dots \\; w_m] And also our input in it's vector representation, [x_1 \\; x_2 \\; x_3 \\; \\dots \\; x_m] [x_1 \\; x_2 \\; x_3 \\; \\dots \\; x_m] And our solution to the linear equation represented as the dot product <w,x> \\; = \\; \\hat{y} <w,x> \\; = \\; \\hat{y} Keep in mind that each of these vectors have length m for the amount of features that we want our machine learning model to use But, what exactly is an \"optimal solution\", and how do we get one? Optimization Optimization is a very important mathematical field in the world of machine learning. Optimization is essentially finding the best values for a problem to either maximize or minimize an \"objective function\". The objective function is a function that we want the output values to be \"optimized\". In our case, the objective function is the loss function , which essentially gives us a measurement of how well our algorithm is doing. Loss functions vary according to the type of problem, but for our case, we are minimizing our Root Mean Square Error (RMSE) , which essentially tells us our average error from our predictions of house value, from the true value. RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} Gradient Descent Let's say we want to find the minimize a function, for example x^2 x^2 . We can easily find the minimum of this function (remember the second derivative rule in calculus?), but how can we do it for more complicated functions that are harder to differentiate like our objective function? This problem will get very hard , and very computationally intensive . To get over this hurdle of finding the exact minimum, we can instead approximate the minimum, using an algorithm called Gradient Descent . Imagine you have a ball and place it on a slope. It will move downwards, with the direction opposite of the gradient of the slope (remember the gradient is in the direction of ascent ). We can think of gradient descent as something similar. Here we will move our current approximation of the minimum, towards the gradient. When the gradient gets small enough (in other words, when the slope is near zero at a minimum) or when we have moved our approximation for enough epochs , then we set the current approximation as our final value. Illustration of Gradient Descent on 2D loss surface Moving down a 3D loss surface However, calculating the gradient of the cost function is a costly procedure, as it has to be done with each weight of your model. If we wanted to do this mathematically, we would have to calculate the equation below to find the derivative for each feature and each sample , which would kill any computer. \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } Instead of doing that, let's simply define the gradient as the difference between our predictions from the current iteration of gradient descent and the true values, multiplied by our sample features. This will get an approximation of the change over each feature the loss. \\nabla f(X) = -X(values-predictions) \\nabla f(X) = -X(values-predictions) Here is psuedocode for Gradient Descent below, courtesy of CS231n: Gradient Descent Pseudocode 4 1 2 3 4 5 # Vanilla Gradient Descent while True : weights_grad = evaluate_gradient ( loss_fun , data , weights ) weights += - step_size * weights_grad # perform parameter update Stochastic Gradient Descent However, gradient descent has a huge drawback: it is computationally ineffecient . For every epoch, the gradient is calculated over all training samples. This is costly, and is unfeasible for most datasets. We can improve this by using Stochastic Gradient Descent (SGD) , a modification of the standard Gradient Descent algorithm. Stochastic Gradient Descent tries to solve the previous problem by only calculating the gradient for a single randomly sampled point at a time , hence the word Stochastic . Thus, this allows to update the weights of our model much quicker, allowing SGD to converge to a minima much quicker than in standard Gradient Descent. Although this optimization method is \"noisier\" it can often be much quicker to optimize a solution (illustrated below). We can reduce this use by calculating the gradient on batches of data instead of a single sample with Minibatch Gradient Descent . This can strike a balance can help reduce noise, but also allow us to still converge on our minima quickly. This noise is not always a negative trait though, as it can in fact possibly find us a more optimal solution for our regression. This additional noise can allow our solution to essentially \"hop out\" of local minima, allowing for the search of a more optimal minima. 5 This can be especially useful when you have a complicated loss landscape with many local minima, such as those for complicated neural networks 6 : Info Approximating the weights of this equation using SGD is one way to solve linear systems, but it is not the only way. One other method that may seem familiar to you is solving the closed form system of equations using linear algebra. I've linked this additional method in the Extras page . Linear Regression in Numpy Alright, let's finally do some coding! Go download the code from this repository, where you will recieve the starter code and As explained above, NumPy and Pandas are tools that every data scientist should know before they embark on solving a problem. In addition to these two, we will also use matplotlib in order to plot graphs related to the performance of our model! 1 2 3 import numpy as np import pandas as pd import matplotlib.pyplot as plt Let's also import our data as well. Load the csv into a dataframe using the function 1 pd . read_csv ( filename ) Info Definition: Dataframe One main feature of the Pandas library is the use of Dataframes. Dataframes make it very easy to filter through large datasets with different types of data. Think of Dataframes as 2D arrays where each element can have different types, or as a SQLite table that you store within your python program. Image of a dataframe that you should see in your Jupyter notebook when you import the data Data Preparation Before we start coding and creating our ML model, we have to do a fair bit of things to our data to ensure the best performance of our model. One major thing in our model is removing missing values in our data. Missing values can affect our function greatly, causing many of the mathematical functions we use to simply not work, so we should remove them! One example is when we are trying to evaluate our linear equation to predict housing values. When we are trying to calculate the prediction, if one feature is an NaN (not a number) in the dataset, we simply cannot compute the prediction! For example, for the linear equation below, when one of the values is a NaN , the output would obviously be NaN as well. w_1*NaN \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; NaN w_1*NaN \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; NaN One way to get around this is to set these features that are missing to 0, but this essentially isn't true, so it is typically best practice to remove such samples that don't have a full feature set, just so these zero's don't affect our model too much. Some useful functions for this are the functions: 1 2 3 4 5 6 #Removes rows with NaN values in the dataframe your_dataframe . dropna () ''' Keep in mind that this functions doesn't edit the original dataframe in place, it returns another dataframe, so make sure to overwrite your dataframe or initialize a new one. ''' Another technique to help us ensure our model performs well is through Normalization of our data. What we mean by normalization is changing the features of our data in set ranges so that not one feature is inherently more important than another. One simple normalization method that we can use is min/max normalization. For this, we put all of our samples in the range of 0 to 1, with 1 being the maximum value of dataset for that feature. This makes our data less sensitive to things like the effect of units for each feature so that each can be weighed more evenly when creating weights for our linear equation. The equation for min/max normalization is given by Normalized\\: Sample = \\frac{Sample}{{Maximum\\: of\\: all\\: samples}} Normalized\\: Sample = \\frac{Sample}{{Maximum\\: of\\: all\\: samples}} 1 2 3 4 5 6 7 8 #TASK: Write a function to normalize all the data in the dataframe with min/max normalization def normalize_data ( df , columns ): #Empty dataframe normalized_df = pd . DataFrame () #ENTER CODE HERE return normalized_df Info Jargon Clarification: Performance This is one thing that is really confusing to a lot of people, how do I know whether my model performs well? The truth is, there isn't really a simple way to know this, and there is a lot of different ways to measure model performance. One way to measure model performance is the accuracy I highly recommend to read up on common machine learning performance metrics, but we will be using average absolute error and mean squared error for our regression, and accuracy for our classifications. Initialization First, let's establish some numbers that we can use to modify the training of our linear regression model. These numbers are called hyperparameters Info Definition: Hyperparameters Hyperparameters define certain portions of your machine learning model, which allows you to customize certain portions of the training to get the best performance as possible. The main hyperparameters we will use during this workshop are the learning rate and the training steps Training steps are the number of times we will update our weights while we run gradient descent. For each training step of gradient descent, we will iterate through the entire dataset. Learning rate the the number that we will multiply our gradient by in each training step. This essentially allows us to decide how large each \"step\" our loss takes during each iteration of gradient descent. In order to find the best hyperparameters, one could do a \"grid search\" , where you could run your model over different combinations of your hyperparameters to get the set that has the best performance. Read more here: https://scikit-learn.org/stable/modules/grid_search.html Since hyperparameters don't matter too much for simple models like linear regression, let's just use something that looks fine. (Honestly a lot of machine learning is trying out different things and seeing if it works \ud83d\ude0a) 1 2 3 4 5 6 7 #Setting up hyperparameters \u200b #Alpha is our learning rate alpha = 0.1 \u200b #Iterations is how many times we will run our algorithm iterations = 10000 Training our Model Now that we have set up our hyperparameters, we can now go over how to train our linear regression model using gradient descent. For this part of the workshop, we are going to create a function that takes in our data, labels, and hyperparameters, and outputs a model and a list containing the mean squared error for each training step. 1 def train ( x , y , training_steps , alpha ): Let's start by initializing our training. In our function, let's define some parameters of our function to ensure that everything works correctly. First, we will set up a history list to store our mean squared error for each iteration (you will see what this is used for later). In addition let's find the amount of weights that we need in addition to the length of our dataset. Functions: 1 2 your_np_array . shape #gets the shape of your python ndarray. #For example, for an nxm ndarray, this will return a tuple containing (n,m) Here is the skeleton code for this section to get you started on the structure 1 2 3 4 5 6 #Storing our cost function, Mean Squared Error history = [] #Finding the number of weights we need and also the number of samples num_weights = #Find the number of weights for n = #Find the number of samples in the dataset Next, let's initialize our weights! Let's initialize these weights randomly so that our model can converge to a minima with a more independent start. A useful function for this is the np.random.rand() function, which creates a list of random numbers. 1 2 3 #Initializing our weights to random numbers np . random . seed ( 69420 ) weights = #initialize weights Now that we have everything set up, I'll leave this part a bit more open ended, and write the pseudocode for each part of the gradient descent loop. For your convenience, here are the equations that are used in the gradient descent loop. Error = y - \\hat{y} Error = y - \\hat{y} \\hat{y} \\; = w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_nx_n \\hat{y} \\; = w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_nx_n \\nabla =(\\frac{1}{n})(y - \\hat{y})x \\nabla =(\\frac{1}{n})(y - \\hat{y})x RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} 1 2 3 4 5 6 7 8 #iterating through each training step for i in range ( iterations ): #testing the model and find the error #finding the root mean square error of the current weights and then append it to our mse list #finding the gradient and then defining the new weights using this gradient return weights , history Testing your model Now that we have trained our model, let's see how our model performs in terms of fitting our data. Let's graph the RMSE which we stored for each training iteration of our gradient descent. Logically, the RMSE should decrease with each training step given our algorithm is working. You can use the code below to graph the RMSE that is returned from your train() function. 1 2 3 4 5 plt . title ( 'Root Mean Square Error (MSE) over each iteration' ) plt . xlabel ( 'No. of iterations' ) plt . ylabel ( 'RMSE' ) plt . plot ( history ) plt . show () We can also measure our final RMSE and average absolute error as well. We can simply take the dot product between the weights and a the inputs to make our predictions, and Linear Regression Binary Classifier Regression is continuous, so how can we turn this into something that is discrete. In other words, how can we go from our ML model predicting values, to predicting categories? In our case, how can we predict if a given sample is above a certain price? One simple way, is simply changing your prediction value from a continuous variable, to a discrete variable. In other words, we can simply change our y values from something that is continuous, to something that is categorical. We can apply a value of 1 for each \"true\" value, and a value of -1 for each \"false\" value. To get classifications after training our classifier, we can then simply get the sign of our output to receive a positive or negative classification Danger Although this is a simple way of creating a classifier, this isn't be best way to create such a classifier. Here is an example given by Andrew Ng: For the problem listed below, of classifying malignant tumors from tumor size, we can see below that the classifier can perform very well since we can simply say that those with a value above 0.5 are malignant, and those below 0.5 are not. However, what if we introduced an outlier sample that has an extremely large tumor size compared to previously malignant samples? Then we can definitely expect that this classifier would perform poorly. As the line that is fitted to our data would predict many of the tumors to be not malignant. This is one issue with using a linear regression model as a classifier, as it is fairly prone to outliers compared to other classification methods. There are many other ways to do such a classification, as we will be making these algorithms in the next workshop, where we get more into classification! One way of doing this is by using a logistic function, or by creating a hyperplane, such that the definition between malignant and nonmalignant is very clear. Source: Stackexchange In the future, we will go over multi-class classification problems (when you are trying to distinguish between more than 2 classes), but for now we will focus on binary classification. Conclusion So in conclusion, we learned a few things from this workshop What Regression is and how you can apply it to real world challenges Basics of optimization, including Gradient Descent and its modifications How classification models are made and how you can use them Python ndarrays vs. Lists https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference \u21a9 Numpy Internals https://docs.scipy.org/doc/numpy-1.13.0/reference/internals.html \u21a9 Tensorflow Core C++ API https://www.tensorflow.org/api_docs/cc/group/core \u21a9 CS231n: Optimization http://cs231n.github.io/optimization-1/#optimization \u21a9 SGD and Local Minima https://leon.bottou.org/publications/pdf/nimes-1991.pdf \u21a9 Neural Network Loss Landscapes https://www.cs.umd.edu/~tomg/projects/landscapes/ \u21a9","title":"1. Basics"},{"location":"1. Basics/#tools-numpy-and-pandas","text":"","title":"Tools: NumPy and Pandas"},{"location":"1. Basics/#numpy","text":"Numpy is a library that includes many tools for mathematical computations, including a computationally efficient ndarray (much faster than python lists) 1 In addition, many mathematical functions useful in linear algebra are included in NumPy with the numpy.linalg functions. These functions will prove to be pretty useful in the rest of the workshop. If you are familiar with using MATLAB from MATH18, then you should be pretty familiar with the functions that NumPy provides.","title":"Numpy"},{"location":"1. Basics/#pandas","text":"Pandas is a library that includes data structures and data analysis tools for data munging and preparation. Pandas makes it relatively easy to interact and filter through data, with nice convenience functions for plotting and exporting data to SQL databases or CSV's. Faq I heard that python is a very slow language. If it's so slow, why do we use python for such intensive tasks such as Machine Learning and Data Analysis? While it is true that Python in itself is a very slow language, most of the tools that we are using to implement ML algorithms don't use Python to do calculations. Libraries such as Numpy and Tensorflow respectively have C and C++ backends 2 3 , allowing them to be very fast. You can even uses other libraries such as Intel MKL to have hardware optimized linear algebra operations in Numpy if you are especially a speed demon.","title":"Pandas"},{"location":"1. Basics/#downloads","text":"Click below to download the data and starter notebook which includes a frame for you to write your code around. Click to Download Dataset Click to Download Starter Code","title":"Downloads"},{"location":"1. Basics/#the-challenge","text":"Say you are a data scientist working for an investment firm. A client wants to invest their money into california real estate, buying homes in a specific block to airbnb. However, none of the homes are for sale, and the people living inside the homes won't let you appraise their homes because they hate airbnb. How do you find an estimate of their home price using data available to you?","title":"The Challenge"},{"location":"1. Basics/#the-dataset","text":"The dataset that you are given for this challenge is a dataset on California home prices. How can you use this data to predict home prices . Below is a description of the dataset and the features you are given to predict the median home value for a block. Column title Description Range* Datatype longitude A measure of how far west a house is; a higher value is farther west Longitude values range from -180 to +180 Data set min: -124.3 Data set max: -114.3 float64 latitude A measure of how far north a house is; a higher value is farther north Latitude values range from -90 to +90 Data set min: 32.5 Data set max: 42.5 float64 housingMedianAge Median age of a house within a block; a lower number is a newer building Data set min: 1.0 Data set max: 52.0 float64 totalRooms Total number of rooms within a block Data set min: 2.0 Data set max: 37937.0 float64 totalBedrooms Total number of bedrooms within a block Data set min: 1.0 Data set max: 6445.0 float64 population Total number of people residing within a block Data set min: 3.0 Data set max: 35682.0 float64 households Total number of households, a group of people residing within a home unit, for a block Data set min: 1.0 Data set max: 6082.0 float64 medianIncome Median income for households within a block of houses (measured in tens of thousands of US Dollars) Data set min: 0.5 Data set max: 15.0 float64 medianHouseValue Median house value for households within a block (measured in US Dollars) Data set min: 14999.0 Data set max: 500001.0 float64 Info Definition: Features To clarify on the use of the word \"features\" above, features are essentially the traits of data that we use to train our machine learning model . For example, when making a model that can predict home prices, we can use all the data above to help predict the median house value for a block. The housingMedianAge , medianIncome , and all the other columns except for our predictor, medianHouseValue , are our features for our model.","title":"The Dataset"},{"location":"1. Basics/#linear-regression","text":"","title":"Linear Regression:"},{"location":"1. Basics/#what-is-regression","text":"Regression is a powerful tool that can often be used for predicting and values from a dataset. This is often done by creating a line or function, where the evaluation of this function can be used to predict such values. For example, with the regression below, you can predict the value of skin cancer mortality from a variable, in this case state latitude. Regression is Continuous , meaning that it is trying to predict continuous values from the variables, or features that your data has.","title":"What is Regression?"},{"location":"1. Basics/#definition","text":"Linear Regression is the process of taking a line and fitting it to data. What we mean by \"fitting\" is that that we want to make a line that \"approximates\" the data (We'll get more into what we mean by this in a bit). The example above is with two dimensions (for example, if your dataset has 1 independent variable). Essentially, we want to find the weights w for a linear equation such that we can predict the final value from input features. Here is an example of the type of equation we are trying to set up with m independent variables. w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; \\hat{y} w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; \\hat{y} Note Representing this equation in code is hard, so it is best to represent this equation in vector (array) notation We can also put or weights and input in their vector representation, [w_1 \\; w_2 \\; w_3 \\; \\dots \\; w_m] [w_1 \\; w_2 \\; w_3 \\; \\dots \\; w_m] And also our input in it's vector representation, [x_1 \\; x_2 \\; x_3 \\; \\dots \\; x_m] [x_1 \\; x_2 \\; x_3 \\; \\dots \\; x_m] And our solution to the linear equation represented as the dot product <w,x> \\; = \\; \\hat{y} <w,x> \\; = \\; \\hat{y} Keep in mind that each of these vectors have length m for the amount of features that we want our machine learning model to use But, what exactly is an \"optimal solution\", and how do we get one?","title":"Definition"},{"location":"1. Basics/#optimization","text":"Optimization is a very important mathematical field in the world of machine learning. Optimization is essentially finding the best values for a problem to either maximize or minimize an \"objective function\". The objective function is a function that we want the output values to be \"optimized\". In our case, the objective function is the loss function , which essentially gives us a measurement of how well our algorithm is doing. Loss functions vary according to the type of problem, but for our case, we are minimizing our Root Mean Square Error (RMSE) , which essentially tells us our average error from our predictions of house value, from the true value. RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}}","title":"Optimization"},{"location":"1. Basics/#gradient-descent","text":"Let's say we want to find the minimize a function, for example x^2 x^2 . We can easily find the minimum of this function (remember the second derivative rule in calculus?), but how can we do it for more complicated functions that are harder to differentiate like our objective function? This problem will get very hard , and very computationally intensive . To get over this hurdle of finding the exact minimum, we can instead approximate the minimum, using an algorithm called Gradient Descent . Imagine you have a ball and place it on a slope. It will move downwards, with the direction opposite of the gradient of the slope (remember the gradient is in the direction of ascent ). We can think of gradient descent as something similar. Here we will move our current approximation of the minimum, towards the gradient. When the gradient gets small enough (in other words, when the slope is near zero at a minimum) or when we have moved our approximation for enough epochs , then we set the current approximation as our final value. Illustration of Gradient Descent on 2D loss surface Moving down a 3D loss surface However, calculating the gradient of the cost function is a costly procedure, as it has to be done with each weight of your model. If we wanted to do this mathematically, we would have to calculate the equation below to find the derivative for each feature and each sample , which would kill any computer. \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } \\frac{d}{{dx}}f\\left( x \\right) = \\mathop {\\lim }\\limits_{\\Delta \\to 0} \\frac{{f\\left( {x + \\Delta } \\right) - f\\left( x \\right)}}{\\Delta } Instead of doing that, let's simply define the gradient as the difference between our predictions from the current iteration of gradient descent and the true values, multiplied by our sample features. This will get an approximation of the change over each feature the loss. \\nabla f(X) = -X(values-predictions) \\nabla f(X) = -X(values-predictions) Here is psuedocode for Gradient Descent below, courtesy of CS231n: Gradient Descent Pseudocode 4 1 2 3 4 5 # Vanilla Gradient Descent while True : weights_grad = evaluate_gradient ( loss_fun , data , weights ) weights += - step_size * weights_grad # perform parameter update","title":"Gradient Descent"},{"location":"1. Basics/#stochastic-gradient-descent","text":"However, gradient descent has a huge drawback: it is computationally ineffecient . For every epoch, the gradient is calculated over all training samples. This is costly, and is unfeasible for most datasets. We can improve this by using Stochastic Gradient Descent (SGD) , a modification of the standard Gradient Descent algorithm. Stochastic Gradient Descent tries to solve the previous problem by only calculating the gradient for a single randomly sampled point at a time , hence the word Stochastic . Thus, this allows to update the weights of our model much quicker, allowing SGD to converge to a minima much quicker than in standard Gradient Descent. Although this optimization method is \"noisier\" it can often be much quicker to optimize a solution (illustrated below). We can reduce this use by calculating the gradient on batches of data instead of a single sample with Minibatch Gradient Descent . This can strike a balance can help reduce noise, but also allow us to still converge on our minima quickly. This noise is not always a negative trait though, as it can in fact possibly find us a more optimal solution for our regression. This additional noise can allow our solution to essentially \"hop out\" of local minima, allowing for the search of a more optimal minima. 5 This can be especially useful when you have a complicated loss landscape with many local minima, such as those for complicated neural networks 6 : Info Approximating the weights of this equation using SGD is one way to solve linear systems, but it is not the only way. One other method that may seem familiar to you is solving the closed form system of equations using linear algebra. I've linked this additional method in the Extras page .","title":"Stochastic Gradient Descent"},{"location":"1. Basics/#linear-regression-in-numpy","text":"Alright, let's finally do some coding! Go download the code from this repository, where you will recieve the starter code and As explained above, NumPy and Pandas are tools that every data scientist should know before they embark on solving a problem. In addition to these two, we will also use matplotlib in order to plot graphs related to the performance of our model! 1 2 3 import numpy as np import pandas as pd import matplotlib.pyplot as plt Let's also import our data as well. Load the csv into a dataframe using the function 1 pd . read_csv ( filename ) Info Definition: Dataframe One main feature of the Pandas library is the use of Dataframes. Dataframes make it very easy to filter through large datasets with different types of data. Think of Dataframes as 2D arrays where each element can have different types, or as a SQLite table that you store within your python program. Image of a dataframe that you should see in your Jupyter notebook when you import the data","title":"Linear Regression in Numpy"},{"location":"1. Basics/#data-preparation","text":"Before we start coding and creating our ML model, we have to do a fair bit of things to our data to ensure the best performance of our model. One major thing in our model is removing missing values in our data. Missing values can affect our function greatly, causing many of the mathematical functions we use to simply not work, so we should remove them! One example is when we are trying to evaluate our linear equation to predict housing values. When we are trying to calculate the prediction, if one feature is an NaN (not a number) in the dataset, we simply cannot compute the prediction! For example, for the linear equation below, when one of the values is a NaN , the output would obviously be NaN as well. w_1*NaN \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; NaN w_1*NaN \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_mx_m \\; = \\; NaN One way to get around this is to set these features that are missing to 0, but this essentially isn't true, so it is typically best practice to remove such samples that don't have a full feature set, just so these zero's don't affect our model too much. Some useful functions for this are the functions: 1 2 3 4 5 6 #Removes rows with NaN values in the dataframe your_dataframe . dropna () ''' Keep in mind that this functions doesn't edit the original dataframe in place, it returns another dataframe, so make sure to overwrite your dataframe or initialize a new one. ''' Another technique to help us ensure our model performs well is through Normalization of our data. What we mean by normalization is changing the features of our data in set ranges so that not one feature is inherently more important than another. One simple normalization method that we can use is min/max normalization. For this, we put all of our samples in the range of 0 to 1, with 1 being the maximum value of dataset for that feature. This makes our data less sensitive to things like the effect of units for each feature so that each can be weighed more evenly when creating weights for our linear equation. The equation for min/max normalization is given by Normalized\\: Sample = \\frac{Sample}{{Maximum\\: of\\: all\\: samples}} Normalized\\: Sample = \\frac{Sample}{{Maximum\\: of\\: all\\: samples}} 1 2 3 4 5 6 7 8 #TASK: Write a function to normalize all the data in the dataframe with min/max normalization def normalize_data ( df , columns ): #Empty dataframe normalized_df = pd . DataFrame () #ENTER CODE HERE return normalized_df Info Jargon Clarification: Performance This is one thing that is really confusing to a lot of people, how do I know whether my model performs well? The truth is, there isn't really a simple way to know this, and there is a lot of different ways to measure model performance. One way to measure model performance is the accuracy I highly recommend to read up on common machine learning performance metrics, but we will be using average absolute error and mean squared error for our regression, and accuracy for our classifications.","title":"Data Preparation"},{"location":"1. Basics/#initialization","text":"First, let's establish some numbers that we can use to modify the training of our linear regression model. These numbers are called hyperparameters Info Definition: Hyperparameters Hyperparameters define certain portions of your machine learning model, which allows you to customize certain portions of the training to get the best performance as possible. The main hyperparameters we will use during this workshop are the learning rate and the training steps Training steps are the number of times we will update our weights while we run gradient descent. For each training step of gradient descent, we will iterate through the entire dataset. Learning rate the the number that we will multiply our gradient by in each training step. This essentially allows us to decide how large each \"step\" our loss takes during each iteration of gradient descent. In order to find the best hyperparameters, one could do a \"grid search\" , where you could run your model over different combinations of your hyperparameters to get the set that has the best performance. Read more here: https://scikit-learn.org/stable/modules/grid_search.html Since hyperparameters don't matter too much for simple models like linear regression, let's just use something that looks fine. (Honestly a lot of machine learning is trying out different things and seeing if it works \ud83d\ude0a) 1 2 3 4 5 6 7 #Setting up hyperparameters \u200b #Alpha is our learning rate alpha = 0.1 \u200b #Iterations is how many times we will run our algorithm iterations = 10000","title":"Initialization"},{"location":"1. Basics/#training-our-model","text":"Now that we have set up our hyperparameters, we can now go over how to train our linear regression model using gradient descent. For this part of the workshop, we are going to create a function that takes in our data, labels, and hyperparameters, and outputs a model and a list containing the mean squared error for each training step. 1 def train ( x , y , training_steps , alpha ): Let's start by initializing our training. In our function, let's define some parameters of our function to ensure that everything works correctly. First, we will set up a history list to store our mean squared error for each iteration (you will see what this is used for later). In addition let's find the amount of weights that we need in addition to the length of our dataset. Functions: 1 2 your_np_array . shape #gets the shape of your python ndarray. #For example, for an nxm ndarray, this will return a tuple containing (n,m) Here is the skeleton code for this section to get you started on the structure 1 2 3 4 5 6 #Storing our cost function, Mean Squared Error history = [] #Finding the number of weights we need and also the number of samples num_weights = #Find the number of weights for n = #Find the number of samples in the dataset Next, let's initialize our weights! Let's initialize these weights randomly so that our model can converge to a minima with a more independent start. A useful function for this is the np.random.rand() function, which creates a list of random numbers. 1 2 3 #Initializing our weights to random numbers np . random . seed ( 69420 ) weights = #initialize weights Now that we have everything set up, I'll leave this part a bit more open ended, and write the pseudocode for each part of the gradient descent loop. For your convenience, here are the equations that are used in the gradient descent loop. Error = y - \\hat{y} Error = y - \\hat{y} \\hat{y} \\; = w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_nx_n \\hat{y} \\; = w_1x_1 \\; + \\; w_2x_2 \\; + \\; \\dots \\; + \\; w_nx_n \\nabla =(\\frac{1}{n})(y - \\hat{y})x \\nabla =(\\frac{1}{n})(y - \\hat{y})x RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} RMSE = \\sqrt{(\\frac{1}{2n})\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}} 1 2 3 4 5 6 7 8 #iterating through each training step for i in range ( iterations ): #testing the model and find the error #finding the root mean square error of the current weights and then append it to our mse list #finding the gradient and then defining the new weights using this gradient return weights , history","title":"Training our Model"},{"location":"1. Basics/#testing-your-model","text":"Now that we have trained our model, let's see how our model performs in terms of fitting our data. Let's graph the RMSE which we stored for each training iteration of our gradient descent. Logically, the RMSE should decrease with each training step given our algorithm is working. You can use the code below to graph the RMSE that is returned from your train() function. 1 2 3 4 5 plt . title ( 'Root Mean Square Error (MSE) over each iteration' ) plt . xlabel ( 'No. of iterations' ) plt . ylabel ( 'RMSE' ) plt . plot ( history ) plt . show () We can also measure our final RMSE and average absolute error as well. We can simply take the dot product between the weights and a the inputs to make our predictions, and","title":"Testing your model"},{"location":"1. Basics/#linear-regression-binary-classifier","text":"Regression is continuous, so how can we turn this into something that is discrete. In other words, how can we go from our ML model predicting values, to predicting categories? In our case, how can we predict if a given sample is above a certain price? One simple way, is simply changing your prediction value from a continuous variable, to a discrete variable. In other words, we can simply change our y values from something that is continuous, to something that is categorical. We can apply a value of 1 for each \"true\" value, and a value of -1 for each \"false\" value. To get classifications after training our classifier, we can then simply get the sign of our output to receive a positive or negative classification Danger Although this is a simple way of creating a classifier, this isn't be best way to create such a classifier. Here is an example given by Andrew Ng: For the problem listed below, of classifying malignant tumors from tumor size, we can see below that the classifier can perform very well since we can simply say that those with a value above 0.5 are malignant, and those below 0.5 are not. However, what if we introduced an outlier sample that has an extremely large tumor size compared to previously malignant samples? Then we can definitely expect that this classifier would perform poorly. As the line that is fitted to our data would predict many of the tumors to be not malignant. This is one issue with using a linear regression model as a classifier, as it is fairly prone to outliers compared to other classification methods. There are many other ways to do such a classification, as we will be making these algorithms in the next workshop, where we get more into classification! One way of doing this is by using a logistic function, or by creating a hyperplane, such that the definition between malignant and nonmalignant is very clear. Source: Stackexchange In the future, we will go over multi-class classification problems (when you are trying to distinguish between more than 2 classes), but for now we will focus on binary classification.","title":"Linear Regression Binary Classifier"},{"location":"1. Basics/#conclusion","text":"So in conclusion, we learned a few things from this workshop What Regression is and how you can apply it to real world challenges Basics of optimization, including Gradient Descent and its modifications How classification models are made and how you can use them Python ndarrays vs. Lists https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference \u21a9 Numpy Internals https://docs.scipy.org/doc/numpy-1.13.0/reference/internals.html \u21a9 Tensorflow Core C++ API https://www.tensorflow.org/api_docs/cc/group/core \u21a9 CS231n: Optimization http://cs231n.github.io/optimization-1/#optimization \u21a9 SGD and Local Minima https://leon.bottou.org/publications/pdf/nimes-1991.pdf \u21a9 Neural Network Loss Landscapes https://www.cs.umd.edu/~tomg/projects/landscapes/ \u21a9","title":"Conclusion"},{"location":"2. Classical ML and NLP/","text":"What is Classical Machine Learning? In this bootcamp, when we refer to Classical Machine Learning Algorithms, we simply mean algorithms that are not based on Deep Learning or Neural Network based learning methods. These algorithms have been used for decades, far before the current hype of Machine Learning and Artificial Intelligence Some examples of Classical Machine Learning Algorithms include but are not limited to: Supervised Learning : Naive Bayes, Linear Regression, Logistic Regression, Ridge Regression, K Nearest Neighbors, Support Vector Machine Unsupervised Learning : Mean-shift, DBSCAN, K-Means Info DEFINITION DETOUR : Unsupervised and Supervised Learning Supervised : Supervised learning is unique in that it has training data. The model will learn from labeled data in order to predict something, usually in the form of classification or regression. Unsupervised : Unsupervised learning is when the algorithm attempts to create structure from unlabeled data. This is usually in the form of clustering, but can take other forms, such as learning representations of data. Since Classical Machine Learning Algorithms are often outclassed by Deep Learning algorthims, why should we even use them? The answer is more clear than you think. Simplicity . Classical Machine Learning Methods are often easier to explain and more computationally efficient that Deep Learning Based Approaches, allowing them to be deployed much easier and cheaper than their neural network-based counterparts. In addition, most classical algorithms run directly on the CPU, voiding the need for more costly GPU's. Perform Better with Less Data . Classical Machine Learning algorithms don't need as much data to get good predictions, and in many cases can perform better than neural networks with limited data. scikit-learn Scikit-learn includes off the shelf machine learning algorithms The Challenge You recently got fired from you job as a data scientist from your job as because they discovered that you lied on your resume and couldn't implement linear regression from scratch. You decide to pivot and work for a newspaper, who is trying to figure out the category of each news article to deliver tailored ads. In addition, they want to create a recommender system to recommend articles to readers. How do you do this? The Dataset The dataset that we are using is called the 20 Newsgroups Dataset which contains ~20,000 news documents spread across 20 different types of newsgroups. comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space misc.forsale talk.politics.misc talk.politics.guns talk.politics.mideast talk.religion.misc alt.atheism soc.religion.christian Natural Language Processing In the past workshop, we were dealing with almost exclusively numerical data. This made things pretty easy for us. This allowed us to put these numbers into vectors, which could easily work with our machine learning algorithms. However, with text data, if you wanted to use it with machine learning, we have to answer one main question How can we turn our text data into features that our machine learning model can use? Feature Engineering and NLP Feature Engineering is the process of making Features from our data. Making proper features is essential for any Machine Learning workflow, especially for applications of NLP. Bag of Words One popular way to create proper features for NLP applications is by using the Bag of Words representation of text data. In bag of words, the order of words and grammar is unimportant, as we only care about the amount of times a words has appeared in a certain text. When using bag of words, a matrix would be created with every row is a specific word with the columns being the count of the word or other measurement. Essentially, the array is made of \"bins\" where each measurement is stored. For example, if we wanted to do a bag of words model with counts on this corpus (a corpus is essentially your full collection or dataset of text) This is the first document This document is the second document And this is the third one. Is this the first document? Your final bag of words representation using counts, or your Count Vectorizer , would look like this: and document first is one second the third this 0 1 1 1 0 0 1 0 1 0 2 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 Note You might think that when using the bag of words representation, you will be removing a lot of contextual and grammatical data about text . This is true, but for many large scale text applications such as the one we are doing here, specific grammar and other traits of sentences are irrelevant. For example, if you wanted to make your own version of the UCSD Spam Quarantine, you might find that many emails labeled as spam, would likely have very high frequency of certain words, likely \"buy\", \"free\", or other similar words. On the other hand, grammar might not be as important to the classification as certain words may be. However, this does not mean that not creating features from grammar is a bad idea, as it is very important in many subsections of NLP such as sentiment analysis and speech translation. TF-IDF On the other hand, using a count vectorizer isn't always the best method of creating a bag of words representation for your text . For example, what if one of your text samples is much longer than others which is the case for many datasets? Obviously the counts would be higher, but these words with higher counts would be weighed higher in your model, therefore causing this specific sample to skew your model. We can alter the count vectorization method by instead using a frequency method, where your bag of words representation is based on proportions instead of raw counts. The most common way to do this is through using term frequency - inverse document frequency method, or TF-IDF . Your term frequency can be calculated as: $$TF(t) = \\text{(# of times word t appears in a document) / (Total # of wrods in the document)} $$ IDF calculated as: $$ IDF(t) = \\text{log(Total # of documents / # of documents with word t in it)} $$ And you final TF-IDF for each word calculated as: TF(t) \\cdot IDF(t) TF(t) \\cdot IDF(t) Data Cleaning In addition, when creating features from text, there may likely be words that add little to nothing to the meaning of a text, for example words like \"for\", \"and\", \"the\", or \"a\". We can remove these stopwords from our text to help clean our text from such useless features that can add to the complexity of our model. Info Occam's razor You may have heard Occam's Razor before, which can be paraphrased as \"The simplest solution is most likely the right one.\" This saying can be true in any discipline in engineering, and is especially true with machine learning models. Higher complexity and unneeded features can cause your model to perform, worse, as yur model can start to weigh in these unnecessary features. Additional complexity to machine learning models also adds extra computational cost, meaning higher actual cost when your models are deployed. This reason is why data cleaning and reducing features is necessary to create well performing ML models. Always try to cut out complexity from your model NLP with sklearn Data Cleaning Creating Features Now that we have removed unwanted text, let's now create features from our text data set. As explained earlier, we can use the bag of words representation with TF-IDF to create features that are independent of text size. This is done easily through TfidfVectorizer() in the sklearn.feature_extraction.text module. Classification Now that we created the features for our text, let's go over a new method that we can use to classify our text samples: **Support Vector Machine. Support Vector Machine Support Vector Machine, or SVM is a relatively new, yet powerful classifier. The workings of an SVM classifier is quite simple. It tries to create a hyperplane in the data space such that To give a geometric representation, the hyperplane below can be illustrated in an \\mathbb{R}^2 \\mathbb{R}^2 and \\mathbb{R}^3 \\mathbb{R}^3 feature space. In \\mathbb{R}^2 \\mathbb{R}^2 , the hyperplane can be defined as a line, and above that a plane to denote the classification boundary in \\mathbb{R}^3 \\mathbb{R}^3 . Info What is the kernel trick? This is a common question asked in many ML interviews, and is fairly useful when trying to create a proper hyperplane. Essentially, the kernel trick is transforming your data to a higher dimension in order to create a hyperplane that can linearly separate the samples. For instance, in the image below, we obviously can't create a line that can intersect these data points to separate the two classes. But we can use a symmetric function, or kernel, to transform this data. The Radial Basis Function, or RBF, is commonly used as a kernel function in addition to Gaussian Filters as well. Learn more about kernel functions here 1 2 Model Validation Now time to get into something we haven't talked too much about yet: Model Validation Clustering Wow, we were so into SVM classification that we almost forgot about the reccomender system. Lets try not to get that, as we definitely wouldn't want to get fired from this job as well. Let's start this problem by doing a quick case study on how recommender systems are typically implemented in industry, most notably how Netflix recommends content to its viewers. K-means K-means psuedocode given a list of N vectors [x_1 \\; \\cdots \\; x_N] [x_1 \\; \\cdots \\; x_N] and an initial list of k group representative vectors [z_1 \\; \\cdots \\; z_k] [z_1 \\; \\cdots \\; z_k] repeat until convergence: Partition the vectors into k groups. For each vector i = 1 ... N, assign x_i x_i to the group associated with the nearest representative. Update representatives. For each group j = 1 .. k, set z_j z_j to be the mean of the vectors in group j. ``` CS 229: Kernels https://see.stanford.edu/Course/CS229/39 \u21a9 Wikipedia: Kernel (Statistics) https://en.wikipedia.org/wiki/Kernel_(statistics) \u21a9","title":"2. Classical ML and NLP"},{"location":"2. Classical ML and NLP/#what-is-classical-machine-learning","text":"In this bootcamp, when we refer to Classical Machine Learning Algorithms, we simply mean algorithms that are not based on Deep Learning or Neural Network based learning methods. These algorithms have been used for decades, far before the current hype of Machine Learning and Artificial Intelligence Some examples of Classical Machine Learning Algorithms include but are not limited to: Supervised Learning : Naive Bayes, Linear Regression, Logistic Regression, Ridge Regression, K Nearest Neighbors, Support Vector Machine Unsupervised Learning : Mean-shift, DBSCAN, K-Means Info DEFINITION DETOUR : Unsupervised and Supervised Learning Supervised : Supervised learning is unique in that it has training data. The model will learn from labeled data in order to predict something, usually in the form of classification or regression. Unsupervised : Unsupervised learning is when the algorithm attempts to create structure from unlabeled data. This is usually in the form of clustering, but can take other forms, such as learning representations of data. Since Classical Machine Learning Algorithms are often outclassed by Deep Learning algorthims, why should we even use them? The answer is more clear than you think. Simplicity . Classical Machine Learning Methods are often easier to explain and more computationally efficient that Deep Learning Based Approaches, allowing them to be deployed much easier and cheaper than their neural network-based counterparts. In addition, most classical algorithms run directly on the CPU, voiding the need for more costly GPU's. Perform Better with Less Data . Classical Machine Learning algorithms don't need as much data to get good predictions, and in many cases can perform better than neural networks with limited data.","title":"What is Classical Machine Learning?"},{"location":"2. Classical ML and NLP/#scikit-learn","text":"Scikit-learn includes off the shelf machine learning algorithms","title":"scikit-learn"},{"location":"2. Classical ML and NLP/#the-challenge","text":"You recently got fired from you job as a data scientist from your job as because they discovered that you lied on your resume and couldn't implement linear regression from scratch. You decide to pivot and work for a newspaper, who is trying to figure out the category of each news article to deliver tailored ads. In addition, they want to create a recommender system to recommend articles to readers. How do you do this?","title":"The Challenge"},{"location":"2. Classical ML and NLP/#the-dataset","text":"The dataset that we are using is called the 20 Newsgroups Dataset which contains ~20,000 news documents spread across 20 different types of newsgroups. comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space misc.forsale talk.politics.misc talk.politics.guns talk.politics.mideast talk.religion.misc alt.atheism soc.religion.christian","title":"The Dataset"},{"location":"2. Classical ML and NLP/#natural-language-processing","text":"In the past workshop, we were dealing with almost exclusively numerical data. This made things pretty easy for us. This allowed us to put these numbers into vectors, which could easily work with our machine learning algorithms. However, with text data, if you wanted to use it with machine learning, we have to answer one main question How can we turn our text data into features that our machine learning model can use?","title":"Natural Language Processing"},{"location":"2. Classical ML and NLP/#feature-engineering-and-nlp","text":"Feature Engineering is the process of making Features from our data. Making proper features is essential for any Machine Learning workflow, especially for applications of NLP.","title":"Feature Engineering and NLP"},{"location":"2. Classical ML and NLP/#bag-of-words","text":"One popular way to create proper features for NLP applications is by using the Bag of Words representation of text data. In bag of words, the order of words and grammar is unimportant, as we only care about the amount of times a words has appeared in a certain text. When using bag of words, a matrix would be created with every row is a specific word with the columns being the count of the word or other measurement. Essentially, the array is made of \"bins\" where each measurement is stored. For example, if we wanted to do a bag of words model with counts on this corpus (a corpus is essentially your full collection or dataset of text) This is the first document This document is the second document And this is the third one. Is this the first document? Your final bag of words representation using counts, or your Count Vectorizer , would look like this: and document first is one second the third this 0 1 1 1 0 0 1 0 1 0 2 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 Note You might think that when using the bag of words representation, you will be removing a lot of contextual and grammatical data about text . This is true, but for many large scale text applications such as the one we are doing here, specific grammar and other traits of sentences are irrelevant. For example, if you wanted to make your own version of the UCSD Spam Quarantine, you might find that many emails labeled as spam, would likely have very high frequency of certain words, likely \"buy\", \"free\", or other similar words. On the other hand, grammar might not be as important to the classification as certain words may be. However, this does not mean that not creating features from grammar is a bad idea, as it is very important in many subsections of NLP such as sentiment analysis and speech translation.","title":"Bag of Words"},{"location":"2. Classical ML and NLP/#tf-idf","text":"On the other hand, using a count vectorizer isn't always the best method of creating a bag of words representation for your text . For example, what if one of your text samples is much longer than others which is the case for many datasets? Obviously the counts would be higher, but these words with higher counts would be weighed higher in your model, therefore causing this specific sample to skew your model. We can alter the count vectorization method by instead using a frequency method, where your bag of words representation is based on proportions instead of raw counts. The most common way to do this is through using term frequency - inverse document frequency method, or TF-IDF . Your term frequency can be calculated as: $$TF(t) = \\text{(# of times word t appears in a document) / (Total # of wrods in the document)} $$ IDF calculated as: $$ IDF(t) = \\text{log(Total # of documents / # of documents with word t in it)} $$ And you final TF-IDF for each word calculated as: TF(t) \\cdot IDF(t) TF(t) \\cdot IDF(t)","title":"TF-IDF"},{"location":"2. Classical ML and NLP/#data-cleaning","text":"In addition, when creating features from text, there may likely be words that add little to nothing to the meaning of a text, for example words like \"for\", \"and\", \"the\", or \"a\". We can remove these stopwords from our text to help clean our text from such useless features that can add to the complexity of our model. Info Occam's razor You may have heard Occam's Razor before, which can be paraphrased as \"The simplest solution is most likely the right one.\" This saying can be true in any discipline in engineering, and is especially true with machine learning models. Higher complexity and unneeded features can cause your model to perform, worse, as yur model can start to weigh in these unnecessary features. Additional complexity to machine learning models also adds extra computational cost, meaning higher actual cost when your models are deployed. This reason is why data cleaning and reducing features is necessary to create well performing ML models. Always try to cut out complexity from your model","title":"Data Cleaning"},{"location":"2. Classical ML and NLP/#nlp-with-sklearn","text":"","title":"NLP with sklearn"},{"location":"2. Classical ML and NLP/#data-cleaning_1","text":"","title":"Data Cleaning"},{"location":"2. Classical ML and NLP/#creating-features","text":"Now that we have removed unwanted text, let's now create features from our text data set. As explained earlier, we can use the bag of words representation with TF-IDF to create features that are independent of text size. This is done easily through TfidfVectorizer() in the sklearn.feature_extraction.text module.","title":"Creating Features"},{"location":"2. Classical ML and NLP/#classification","text":"Now that we created the features for our text, let's go over a new method that we can use to classify our text samples: **Support Vector Machine.","title":"Classification"},{"location":"2. Classical ML and NLP/#support-vector-machine","text":"Support Vector Machine, or SVM is a relatively new, yet powerful classifier. The workings of an SVM classifier is quite simple. It tries to create a hyperplane in the data space such that To give a geometric representation, the hyperplane below can be illustrated in an \\mathbb{R}^2 \\mathbb{R}^2 and \\mathbb{R}^3 \\mathbb{R}^3 feature space. In \\mathbb{R}^2 \\mathbb{R}^2 , the hyperplane can be defined as a line, and above that a plane to denote the classification boundary in \\mathbb{R}^3 \\mathbb{R}^3 . Info What is the kernel trick? This is a common question asked in many ML interviews, and is fairly useful when trying to create a proper hyperplane. Essentially, the kernel trick is transforming your data to a higher dimension in order to create a hyperplane that can linearly separate the samples. For instance, in the image below, we obviously can't create a line that can intersect these data points to separate the two classes. But we can use a symmetric function, or kernel, to transform this data. The Radial Basis Function, or RBF, is commonly used as a kernel function in addition to Gaussian Filters as well. Learn more about kernel functions here 1 2","title":"Support Vector Machine"},{"location":"2. Classical ML and NLP/#model-validation","text":"Now time to get into something we haven't talked too much about yet: Model Validation","title":"Model Validation"},{"location":"2. Classical ML and NLP/#clustering","text":"Wow, we were so into SVM classification that we almost forgot about the reccomender system. Lets try not to get that, as we definitely wouldn't want to get fired from this job as well. Let's start this problem by doing a quick case study on how recommender systems are typically implemented in industry, most notably how Netflix recommends content to its viewers.","title":"Clustering"},{"location":"2. Classical ML and NLP/#k-means","text":"K-means psuedocode given a list of N vectors [x_1 \\; \\cdots \\; x_N] [x_1 \\; \\cdots \\; x_N] and an initial list of k group representative vectors [z_1 \\; \\cdots \\; z_k] [z_1 \\; \\cdots \\; z_k] repeat until convergence: Partition the vectors into k groups. For each vector i = 1 ... N, assign x_i x_i to the group associated with the nearest representative. Update representatives. For each group j = 1 .. k, set z_j z_j to be the mean of the vectors in group j. ``` CS 229: Kernels https://see.stanford.edu/Course/CS229/39 \u21a9 Wikipedia: Kernel (Statistics) https://en.wikipedia.org/wiki/Kernel_(statistics) \u21a9","title":"K-means"},{"location":"3. Neural Networks and Computer Vision/","text":"What are Neural Networks Neural Networks differ a lot compared to the past methods in that the inspiration is not mathematical, but biological. Neural networks operate in similar way TensorFlow Tensorflow Keras","title":"3. Neural Networks and Computer Vision"},{"location":"3. Neural Networks and Computer Vision/#what-are-neural-networks","text":"Neural Networks differ a lot compared to the past methods in that the inspiration is not mathematical, but biological. Neural networks operate in similar way","title":"What are Neural Networks"},{"location":"3. Neural Networks and Computer Vision/#tensorflow","text":"Tensorflow","title":"TensorFlow"},{"location":"3. Neural Networks and Computer Vision/#keras","text":"","title":"Keras"},{"location":"Additional Resources/","text":"Slideshows for Previous Workshops: Additional Machine Learning Resources if you are interested: Stanford Courses: These courses are taught by some of the world's most prevalent experts in machine learning, and include all information from lecture videos and notes to get you started. I highly reccomend these to those starting out. CS 230: Deep Learning - Stanford University CS 224N: Natural Language Processing with Deep Learning - Stanford University CS 231n: Convolutional Neural networks for Visual Recognition - Stanford University Online Courses Udacity Intro to Machine Learning fast.ai Practical Deep Learning for Coders Google Machine Learning Crash Course Machine Learning Videos sentdex ML tutorials Interview Prep: Machine Learning Flashcards","title":"Additional Resources"},{"location":"Additional Resources/#slideshows-for-previous-workshops","text":"","title":"Slideshows for Previous Workshops:"},{"location":"Additional Resources/#additional-machine-learning-resources-if-you-are-interested","text":"","title":"Additional Machine Learning Resources if you are interested:"},{"location":"Additional Resources/#stanford-courses","text":"These courses are taught by some of the world's most prevalent experts in machine learning, and include all information from lecture videos and notes to get you started. I highly reccomend these to those starting out. CS 230: Deep Learning - Stanford University CS 224N: Natural Language Processing with Deep Learning - Stanford University CS 231n: Convolutional Neural networks for Visual Recognition - Stanford University","title":"Stanford Courses:"},{"location":"Additional Resources/#online-courses","text":"Udacity Intro to Machine Learning fast.ai Practical Deep Learning for Coders Google Machine Learning Crash Course","title":"Online Courses"},{"location":"Additional Resources/#machine-learning-videos","text":"sentdex ML tutorials","title":"Machine Learning Videos"},{"location":"Additional Resources/#interview-prep","text":"Machine Learning Flashcards","title":"Interview Prep:"},{"location":"Extras/","text":"Workshop 1 - Linear Algebra Method of Solving the Problem of Least Squares Some of you may have asked, why would you use SGD when other methods for solving for least squares exist, such as simply finding the psuedoinverse? This is a very good question, as solving for the least squares problem can be easily done through the psuedoinverse. In the section below, I'll explain How you can use the psuedoinverse to find the weights of a linear least squares problem Why this method is in many cases can be worse than using SGD to find the weights. Bridging the Gap: Linear Algbra and Least Squares Let's get into the math of it. Say if we wanted to find the line between two points in a two dimensional space i.e. we only want to fit a line to two points in our dataset. This would be easy, simply solve the system of linear equations to find a line that intersects these two points. In other words, we can just row reduce the matrix below. rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} or, solving for x, the parameters of the line which predicts y, is the following: x = A^{-1}y x = A^{-1}y Therefore, we can get a line such that mx = y What about if we have so many vectors that we aren't able to make a line that intersects these points, for example with the dataset we've shown above? Row reducing this matrix is impossible as the resulting augmented matrix will have free row variables, meaning there is no solution and the matrix is not invertible (Keep in mind that a matrix needs to be invertible in order to find x). rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} This type of matrix is known as a tall, or overdetermined matrix. However, even though there is no exact solution because the matrix isn't invertible, that doesn't mean that there isn't an approximate solution that we can solve for. Psuedoinverse: Solving Overdetermined Matrices One way to solve for this is finding the psuedoinverse of the matrix to solve for the closest solution. The pseudoinverse is found with the following: We want to invert the right side in order to isolate x. If A was square, we could simply multiply both sides by the inverse of the original matrix to find our weights, x. However, we can make the matrix square through a simple algebraic transformation. We can multiply both sides by the matrix transposed, A^T A^T , such that A^TAx = A^Ty A^TAx = A^Ty We can then multiply both sides by the inverse of the new matrix on the left, (A^TA)^{-1} (A^TA)^{-1} such that x = (A^TA)^{-1}(A^TA)y x = (A^TA)^{-1}(A^TA)y This will get us the weights of the line, x, that can be used to predict values. Summary Just to give a bit of explanations for the variables we used above: \\hat{y} \\hat{y} : The predicted value that approximates y for an input sample y y : The true value for each sample that we are trying to predict A A : The input matrix. i.e. the features as the columns with each row being a single sample of your data. x x : The weights of the final approximate least squares equation","title":"Extras"},{"location":"Extras/#workshop-1-linear-algebra-method-of-solving-the-problem-of-least-squares","text":"Some of you may have asked, why would you use SGD when other methods for solving for least squares exist, such as simply finding the psuedoinverse? This is a very good question, as solving for the least squares problem can be easily done through the psuedoinverse. In the section below, I'll explain How you can use the psuedoinverse to find the weights of a linear least squares problem Why this method is in many cases can be worse than using SGD to find the weights.","title":"Workshop 1 - Linear Algebra Method of Solving the Problem of Least Squares"},{"location":"Extras/#bridging-the-gap-linear-algbra-and-least-squares","text":"Let's get into the math of it. Say if we wanted to find the line between two points in a two dimensional space i.e. we only want to fit a line to two points in our dataset. This would be easy, simply solve the system of linear equations to find a line that intersects these two points. In other words, we can just row reduce the matrix below. rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22}\\end{bmatrix} = \\begin{bmatrix}w_{1} & 0\\\\0 & w_{2}\\end{bmatrix} or, solving for x, the parameters of the line which predicts y, is the following: x = A^{-1}y x = A^{-1}y Therefore, we can get a line such that mx = y What about if we have so many vectors that we aren't able to make a line that intersects these points, for example with the dataset we've shown above? Row reducing this matrix is impossible as the resulting augmented matrix will have free row variables, meaning there is no solution and the matrix is not invertible (Keep in mind that a matrix needs to be invertible in order to find x). rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} rref\\begin{bmatrix}a_{11} & a_{12}\\\\a_{21} & a_{22} \\\\a_{31} & a_{32} \\\\... & ... \\\\a_{n1} & a_{n2} \\end{bmatrix} This type of matrix is known as a tall, or overdetermined matrix. However, even though there is no exact solution because the matrix isn't invertible, that doesn't mean that there isn't an approximate solution that we can solve for.","title":"Bridging the Gap: Linear Algbra and Least Squares"},{"location":"Extras/#psuedoinverse-solving-overdetermined-matrices","text":"One way to solve for this is finding the psuedoinverse of the matrix to solve for the closest solution. The pseudoinverse is found with the following: We want to invert the right side in order to isolate x. If A was square, we could simply multiply both sides by the inverse of the original matrix to find our weights, x. However, we can make the matrix square through a simple algebraic transformation. We can multiply both sides by the matrix transposed, A^T A^T , such that A^TAx = A^Ty A^TAx = A^Ty We can then multiply both sides by the inverse of the new matrix on the left, (A^TA)^{-1} (A^TA)^{-1} such that x = (A^TA)^{-1}(A^TA)y x = (A^TA)^{-1}(A^TA)y This will get us the weights of the line, x, that can be used to predict values. Summary Just to give a bit of explanations for the variables we used above: \\hat{y} \\hat{y} : The predicted value that approximates y for an input sample y y : The true value for each sample that we are trying to predict A A : The input matrix. i.e. the features as the columns with each row being a single sample of your data. x x : The weights of the final approximate least squares equation","title":"Psuedoinverse: Solving Overdetermined Matrices"}]}